\documentclass[12pt, oneside]{article}
\usepackage{a4wide}
\usepackage{oldgerm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\setlength{\textheight}{8.875in} \setlength{\textwidth}{6.875in}
\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{0in}
\setlength{\parindent}{1pc} \setlength{\oddsidemargin}{-.304in}
\setlength{\evensidemargin}{-.304in}



\begin{document}
\setlength{\textheight}{8.5in}
\centering {\bf MTL 106 (Introduction to Probability Theory and Stochastic Processes) }\\


\centering{\bf Assignment 1 Report}



\vskip 0.5cm

\noindent Name: Arpit Saxena ~~~~~~~~~~~~~~~~~~~~~ Entry Number: 2018MT10742



\vskip 0.5cm



\begin{enumerate}
	



\item {
    Basic Probability
}


\item {
    Random Variable/Function of a Random Variable

    Alice is trying to send \(X\) bits of data to Bob, where \(X \sim P(5)\). However, during
    transmission there is a \(10\%\) chance for each bit to flip. What is the probability
    that Bob receives incorrect data? Now suppose Alice also sends a parity bit, which is
    0 if there are even number of bits equal to 1, and 1 otherwise; and Bob then checks the
    data with the parity bit upon receiving i.e. if he receives data with 3 bits set and
    parity bit 0, he'll know the data is erroneous. What is the probability the Bob receives
    data which is erroneous and also matches the information given by the parity bit?

    \textbf{Solution}

    \underline{Without the parity bit}

    If \(X = k\), then the probability of successful transmission is \((1 - 0.1)^k = 0.9 ^ k\),
    which means probability of error in transmission is \(1 - 0.9^k\)

    \(\therefore\) By the total probability rule,
    \begin{align*}
        P(\text{error in transmission}) &= \sum_{k = 0}^{\infty} 
                        P(\text{error in transmission } |\,X = k) \times P(X = k) \\
            &= \sum_{k = 0}^{\infty} \{1 - 0.9^k\} \frac{e^{-5}\,5^k}{k!} \\
            &= \sum_{k = 0}^{\infty} \frac{e^{-5}\,5^k}{k!} - \sum_{k = 0}^{\infty} 0.9^k\,\frac{e^{-5}\,5^k}{k!} \\
            &= e^{-5}\,\sum_{k = 0}^{\infty} \frac{5^k}{k!} - \sum_{k = 0}^{\infty} \frac{e^{-5}\,4.5^k}{k!} \\
            &= 1 - e^{-5}\,e^{0.9 \times 5}  \tag*{(Using Taylor series of \(e^x\))} \\
            &= 1 - e^{-0.5} \\
            &\approxeq 0.39
    \end{align*}

    \underline{With the parity bit}

    Let \(p = 0.1\) be the chance that a bit flips

    If \(X = k\), then there are two cases to consider, namely, if the parity bit changes
    during transmission or the parity bit remains the same.

    \begin{enumerate}
        \item {
            Parity bit remains the same:

            For the data to be changed and still match the information given by the parity bit,
            we note that an non-zero and even number of bits must be flipped, so the number of
            bits equal to 1 modulo 2 remains the same.

            So, in this case,
            \[
                P(\text{undetectable error}) = {k \choose 2}\,p^2\,(1-p)^{k-2} + {k \choose 4}\,p^4\,(1-p)^{k-4} + \cdots
            \]
        }
        \item {
            Parity bit flips:

            For the data to be changed and match the parity bit received by Bob, we observe that
            an even number of bits in the data must be flipped, so the number of bits equal to 1
            modulo 2 changes from the original data

            \(\therefore\) In this case,
            \[
                P(\text{undetectable error}) = {k \choose 1}\,p^1\,(1-p)^{k-1} + {k \choose 3}\,p^3\,(1-p)^{k-3} + \cdots
            \]
        }
    \end{enumerate}

    We note that the probability of the parity bit flipping is \(p\), and so combining the cases,
    \begin{align}
        P(\text{undetectable error}) &= (1 - p) \,
            \left[{k \choose 2}\,p^2\,(1-p)^{k-2} + {k \choose 4}\,p^4\,(1-p)^{k-4} + \cdots\right] \nonumber \\
            &+ p \,
            \left[{k \choose 1}\,p^1\,(1-p)^{k-1} + {k \choose 3}\,p^3\,(1-p)^{k-3} + \cdots\right]
            \label{eq:1}
    \end{align}

    To solve these equations, we use the binomial theorem as follows:
    \begin{align*}
        (x + y)^k &= {k \choose 0}\,x^0\,y^k + {k \choose 1}\,x^1\,y^{k-1} + \cdots + {k \choose k}\,x^k\,y^0 \\
        (y - x)^k &= {k \choose 0}\,x^0\,y^k - {k \choose 1}\,x^1\,y^{k-1} + \cdots + (-1)^k \, {k \choose k}\,x^k\,y^0 
    \end{align*}

    Adding and subtracting the above two equations, and replacing \(x\) by p and \(y\) by \(1-p\), we get
    \begin{align}
        \frac{1}{2}\left\{(p + 1 - p)^k + (1 - p - p)^k\right\} &= {k \choose 0}\,p^0\,(1-p)^k + {k \choose 2}\,p^2\,(1-p)^{k-2} + \cdots \label{eq:2}\\
        \frac{1}{2}\left\{(p + 1 - p)^k - (1 - p - p)^k\right\} &= {k \choose 1}\,p^1\,(1-p)^{k-1} + {k \choose 3}\,p^3\,(1-p)^{k-3} + \cdots \label{eq:3}
    \end{align}

    From equations \ref{eq:1}, \ref{eq:2} and \ref{eq:3}, we get
    \begin{align*}
        P(\text{undetectable error}) &= (1 - p)\,\left[\frac{1}{2}\left\{1 + (1 - 2p)^k\right\} - 
                {k \choose 0}\,p^0\,(1-p)^k\right] \\
                &+ p\,\left[\frac{1}{2}\left\{1 - (1 - 2p)^k\right\}\right] \\
            &= \frac{1}{2} - (1 - p)^{k + 1} + \frac{1}{2}\,(1 - 2p)^{k+1} \\
        \intertext{Replacing \(p\) by 0.1, we get}
        P(\text{undetectable error}) &= \frac{1}{2} - 0.9^{k + 1} + \frac{1}{2}\,0.8^{k+1}
    \end{align*}

    Now by the total probability rule,
    \begin{align*}
        P(\text{undetected error}) &= \sum_{k = 0}^{\infty} 
                        P(\text{undetected error } |\,X = k) \times P(X = k) \\
            &= \sum_{k = 0}^{\infty} 
                \left[\frac{1}{2} - 0.9^{k + 1} + \frac{1}{2}\,0.8^{k+1}\right]
                \frac{e^{-5}\,5^k}{k!} \\
            &= \frac{1}{2}e^{-5}\sum_{k = 0}^{\infty}\frac{5^k}{k!} 
               - 0.9 \times e^{-5} \sum_{k = 0}^{\infty} \frac{4.5^k}{k!}
               + 0.8 \times \frac{1}{2} e^{-5} \sum_{k = 0}^{\infty} \frac{4^k}{k!} \\
            &= \frac{1}{2} - 0.9 \times e^{-5}\,e^{4.5} + 0.4 \times e^{-5}\,e^{4} \tag*{(Using Taylor series of \(e^x\))}\\
            &= \frac{1}{2} - 0.9 \times e^{-0.5} + 0.4 \times e^{-1} \\
            &\approxeq  0.10
    \end{align*}
}

\item Two Dimensional Random Variables

\item Two Dimensional Random Variables

\item {
    Higher Dimensional Random Variables

    \begin{enumerate}
        \item Let \(A, B, C\) be 3 random variables such that their pdf is given by the function
        \[
            f(a, b, c) = \begin{cases}
                            1 & 0 < a < 1, 0 < b < 1, 0 < c < 1 \\
                            0 & \text{otherwise}
                         \end{cases}
        \]
        Find the CDF of \(A^2 + B^2 - C\) in terms of \(f\)
        \item Consider the equation \(x^2 + y^2 + 2Ax + 2By + C = 0\). Find the probability
        that this equation represents a real circle (i.e. radius \(> 0\)).
    \end{enumerate}

    \textbf{Solution}

    Let \(R = A^2 + B^2 - C\)

    We note that \(min(R) = -1\) and \(max(R) = 1\) since the maximum and minimum values
    of \(A, B, C\) are 1 and 0 respectively

    \(\therefore P(R \leq -1) = 0 \text{ and } P(R \leq 1) = 1\)

    Since \(A^2 + B^2 - C <= r \implies A^2 + B^2 <= C + r\), it makes sense to make
    cases on \(r\) with breakpoints \(r = -1\) since it gives a lower limit to the value
    \(C\) could take, \(r = 0\) since it limits the value of \(C + r\), and then finally
    at \(r = 1\)

    \begin{enumerate}
        \item {
            \(-1 \leq r < 0\)

            Since \(A^2 + B^2 = C + r\), it must be positive. So \(C\) must range from
            \(1 - r\) to \(1\). 

            For \(C = c\) in the said range, \(0 \leq c + r < 1\) and thus \(A\) can
            range from 0 to \(c + r\) and similarly for \(B\).

            \(\therefore\) In this case,
            \[
                P(R \leq r) = \int_{c = -r}^{1} \int_{a=0}^{\sqrt{c + r}} 
                              \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
            \]
        }

        \item {
            \(0 \leq r < 1\)

            \(A^2 + B^2 \leq C + r\) gives a breakpoint at \(1 - r\) for \(C\) since
            for the former, \(A\) can go from \(0 \text{ to } c + r\) but not for the
            latter since \(c + r\) would become greater than 1.

            \begin{itemize}
                \item {
                    \(0 < c < 1 - r\)

                    As described above, for this case, \(A\) will range from 0 to 
                    \(\sqrt{c + r}\) and similarly for B.
                    So the required probability is:
                    \[
                        \int_{c = 0}^{1 - r} \int_{a=0}^{\sqrt{c + r}} 
                        \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
                    \]
                }

                \item {
                    \(1 - r \leq c < 1\)

                    For this case \(c + r > 1\), and \(A\) can range fully from 0 to 1
                    Then, considering \(B^2 \leq c + r - a^2\) we note that when 
                    \(a > \sqrt{c + r - 1}\), \(B\) ranges from 0 to \(\sqrt{c + r - a^2}\)
                    and when \(0 < a leq \sqrt{c + r - 1}\), \(B\) ranges from 0 to 1.

                    \(\therefore\) The required probability is:
                    \[
                        \int_{c = 1 - r}^{1} \int_{a=0}^{\sqrt{c + r - 1}} 
                        \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc
                        + \int_{c = 1 - r}^{1} \int_{\sqrt{c + r - 1}}^{1} 
                        \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
                    \]
                }
            \end{itemize}

            Totalling up the probabilities for this case, we have
            \begin{align*}
                P(R \leq r) &= \int_{c = 0}^{1 - r} \int_{a=0}^{\sqrt{c + r}} 
                \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc \\
                &+ \int_{c = 1 - r}^{1} \int_{a=0}^{\sqrt{c + r - 1}} 
                \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc \\
                &+ \int_{c = 1 - r}^{1} \int_{\sqrt{c + r - 1}}^{1} 
                \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
            \end{align*}
        }

        \item {
            \(1 \leq r < 2\)

            \(A^2 + B^2 \leq C + r\) gives the breakpoint at \(2 - r\) for \(C\) since
            for the part before the breakpoint, \(A\) and \(B\) can't take full values,
            but for \(c \geq 2 - r\), we get \(c + r \geq 2\) which means both \(A\) and \(B\)
            can be from 0 to 1.

            \begin{itemize}
                \item {
                    \(0 < c < 2 - r\)

                    For this case, \(1 \leq c + r < 2\), so \(A\) can fully range from
                    0 to 1. However, from \(B^2 \leq c + r - A^2\), when \(A\) is from 0 to
                    \(\sqrt{c + r - 1}\), \(B\) can fully range from 0 to 1 but not in the
                    remaining interval.

                    \begin{itemize}
                        \item {
                            \(0 < a < \sqrt{c + r - 1}\)

                            As noted above, \(B\) can range from 0 to 1, so the required probability
                            is:
                            \[
                                \int_{c = 0}^{2 - r} \int_{a=0}^{\sqrt{c + r - 1}} 
                                \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc
                            \]
                        }
                        \item {
                            \(\sqrt{c + r - 1} \leq a < 1\)

                            Here, \(c + r - a^2 \leq 1\), so \(B\) will range from 0 to
                            \(\sqrt{c + r - a^2}\). So the required probability is:
                            \[
                                \int_{c = 0}^{2 - r} \int_{a=\sqrt{c + r - 1}}^{1} 
                                \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
                            \]
                        }
                    \end{itemize}
                }
                \item {
                    \(2 - r \leq c < 1\)

                    Here, as we already noted, \(c + r \geq 2\), so \(A\) and \(B\) can fully
                    range from 0 to 1. So the required probability is:

                    \[
                        \int_{c = 2 - r}^{1} \int_{a=0}^{1} 
                        \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc
                    \]
                }
            \end{itemize}

            Totalling up the probabilities for this case, we have
            \begin{align*}
                P(R \leq r) &= \int_{c = 0}^{2 - r} \int_{a=0}^{\sqrt{c + r - 1}} 
                                \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc \\
                            &+ \int_{c = 0}^{2 - r} \int_{a=\sqrt{c + r - 1}}^{1} 
                                \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc \\
                            &+ \int_{c = 2 - r}^{1} \int_{a=0}^{1} 
                                \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc
            \end{align*}
        }
    \end{enumerate}

    The probabilities as calculated in the different cases can be stated together and
    thus would define the CDF completely.

    Given the equation \(x^2 + y^2 + 2Ax + 2By + C = 0\), we can rearrange it to
    \((x - A)^2 + (y - B)^2 = A^2 + B^2 - C\), which as we observe is a circle with the
    centre \((A, B)\) and square of the radius equal to \(A^2 + B^2 - C\)

    We need to find \(P(A^2 + B^2 - C > 0)\). Taking the expression of \(P(R \leq r)\)
    corresponding to \(r = 0\) and substituting \(r = 0\), we get
    \begin{align*}
        P(A^2 + B^2 - C \leq 0) &= \int_{c = 0}^{1} \int_{a=0}^{\sqrt{c}} 
                                    \int_{b = 0}^{\sqrt{c - a^2}} f(a, b, c)\, db\, da\, dc \\
                                &= \int_{0}^{1} \int_{0}^{\sqrt{c}} 
                                \int_{0}^{\sqrt{c - a^2}} \, db\, da\, dc \\
                                &= \int_{0}^{1} \int_{0}^{\sqrt{c}} \sqrt{c - a^2}\, da\, dc \\
                                &= \int_{0}^{1} \left[\frac{a}{2}\sqrt{c - a^2} + \frac{c}{2}
                                    \sin^{-1}{\frac{a}{\sqrt{c}}}\right]^{\sqrt{c}}_{0} dc \\
                                &= \int_{0}^{1} \frac{c}{2} \, \frac{\pi}{2} \, dc \\
                                &= \frac{\pi}{8}
    \end{align*}

    \(\therefore P(A^2 + B^2 - C > 0) = 1 - P(A^2 + B^2 - C \leq 0) = 1 - \dfrac{\pi}{8}\)
}

\item Higher Dimensional Random Variables

\item {
    Cross Moments

    Two points are chosen randomly on the perimeter of a square with sides of unit length.
    Find the expected value of the distance between the two points.

    \textbf{Solution}

    We observe that there can be 3 cases for the points with respect to the positions at
    which they lie on the perimeter, namely, they can lie either on the same side, adjacent
    sides or on opposite sides.

    In the following cases, we calculate the conditional expectation of the distance between
    the points given that case:

    \begin{enumerate}
        \item {
            Points lie on the same side

            Let \(E_1\) be the event that both points, say \(A\) and \(B\), lie on
            the same side of the square. Let their distance from an endpoint of the edge
            be \(X\) and \(Y\) respectively, so distance between them is \(|X - Y|\)

            We observe that the conditional probability distribution function,
            \(p_{X, Y / E_1} = 1\) 

            Then, \[
                E(\text{Distance between the points} \,|\, E_1) = \int_{0}^{1} \int_{0}^{1}
                   |x - y| \,dy \,dx = \frac{1}{3}
            \]
        }

        \item {
            Points lie on adjacent sides

            Let \(E_2\) be the event that both points, say \(A\) and \(B\), lie on
            adjacent sides of the square. Let their distance from the vertex common to
            both the edges be \(X\) and \(Y\) respectively, so distance between them is
            \(\sqrt{X^2 + Y^2}\)

            We observe that the conditional probability distribution function,
            \(p_{X, Y / E_1} = 1\) 

            Then. \[
                E(\text{Distance between the points} \,|\, E_2) = \int_{0}^{1} \int_{0}^{1}
                \sqrt{x^2 + y^2} \,dy \,dx \approx 0.7652
            \]
            \hfill (Note: Calculated using numerical integration tools)
        }

        \item {
            Points lie on opposite sides

            Let \(E_3\) be the event that both points, say \(A\) and \(B\), lie on
            opposite sides of the square. Let their distance from the vertices on the
            same side be \(X\) and \(Y\) respectively, so distance between them is
            \(\sqrt{(X - Y)^2 + 1}\)

            We observe that the conditional probability distribution function,
            \(p_{X, Y / E_1} = 1\) 

            Then, \[
                E(\text{Distance between the points} \,|\, E_3) = \int_{0}^{1} \int_{0}^{1}
                   \sqrt{(x-y)^2 + 1} \,dy \,dx \approx 1.0766
            \]
            \hfill (Note: Calculated using numerical integration tools)
        }
    \end{enumerate}

    Now, we calculate the probabilities of the events \(E_1, E_2, E_3\)

    \begin{enumerate}
        \item {
            \(E_1\) is the event that both points lie on the same side of the square.
            The first point can be on any side and for the second point to be on
            the same side, the probability is \(\frac{1}{4}\)

            \[\therefore P(E_1) = \frac{1}{4}\]
        }
        \item {
            \(E_2\) is the event that both points lie on adjacent sides of the square.
            The first point can be on any side and the second point has 2 choices of
            adjacent sides, so the probability is \(\frac{2}{4} = \frac{1}{2}\)

            \[\therefore P(E_2) = \frac{1}{2}\]
        }
        \item {
            \(E_3\) is the event that both points lie on opposite sides of the square.
            The first point can be on any side and the second point has 1 choice for an
            opposite side, so the probability is \(\frac{1}{4}\)

            \[\therefore P(E_3) = \frac{1}{4}\]
        }
    \end{enumerate}

    Now, we calculate the expected distance between the two points
    \begin{align*}
        E(\text{Distance between the points}) &= P(E_1) \cdot E(\text{Distance between the points} \,|\, E_1) \\
                                              &+ P(E_2) \cdot E(\text{Distance between the points} \,|\, E_2) \\
                                              &+ P(E_3) \cdot E(\text{Distance between the points} \,|\, E_3) \\
                                              &\approx \frac{1}{4} \times \frac{1}{3} + \frac{1}{2} \times 0.7652
                                                   + \frac{1}{4} \times 1.0766 \\
                                              &\approx 0.7351
    \end{align*}
}

\item Cross Moments

\item Limiting Distributions

\item Limiting Distributions



\end{enumerate}

\end{document}
