\documentclass[12pt, oneside]{article}
\usepackage{a4wide}
\usepackage{oldgerm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\setlength{\textheight}{8.875in} \setlength{\textwidth}{6.875in}
\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{0in}
\setlength{\parindent}{1pc} \setlength{\oddsidemargin}{-.304in}
\setlength{\evensidemargin}{-.304in}



\begin{document}
\setlength{\textheight}{8.5in}
\centering {\bf MTL 106 (Introduction to Probability Theory and Stochastic Processes) }\\


\centering{\bf Assignment 1 Report}



\vskip 0.5cm

\noindent Name: Arpit Saxena ~~~~~~~~~~~~~~~~~~~~~ Entry Number: 2018MT10742



\vskip 0.5cm



\begin{enumerate}
	



\item {
    Basic Probability

    Let the sample space, \(\Omega = \mathbb{N}\). Define a \(\sigma\)-field on \(\Omega\)
    which has infinite events, and is neither the smallest nor the largest \(\sigma\)-field.
    Define a probability on that \(\sigma\)-field with an infinite number of non zero values

    \textbf{Solution}

    Let \(\mathcal{F}\) include sets whose elements are powers of 2 and their complements.
    
    Since all elements of \(\varnothing\) are powers of 2, \(\varnothing \in \mathcal{F}\) 
    and since complements of the set are also included, \(\Omega \in \mathcal{F}\)

    Let \(A_i, i = 1, 2, \ldots \in \mathcal{F}\). Then:
    \begin{enumerate}
        \item {
            All the sets contain only powers of 2:

            In this case, the union of the sets would also include only powers of 2,
            and thus \(\bigcup_{i}A_i \in \mathcal{F}\)
        }
        \item {
            There is atleast one set which contains an element which is not a power of 2:

            Since that set does not contain all powers of 2, it must be the complement
            of a set containing powers of 2 (since those are the only type of sets included
            in \(\mathcal{F}\)). 

            So, we have one set which is of the form \(\mathbb{R} \setminus \{2^{i_1}, 2^{i_2}, \ldots\}\)
            When we union this with sets which only contain powers of 2, it stays in the form
            of a complement of a set. Same thing happens when we union sets which are complements 
            of sets containing only powers of 2.

            Therefore, the union of these sets would be of the form \(\mathbb{R} \setminus \{2^{i_1}, 2^{i_2}, \ldots\}\)
            where \(i_1, i_2, \ldots \in \mathbb{N} \cup \{0\}\) and thus \(\bigcup_{i}A_i \in \mathcal{F}\)
        }
    \end{enumerate}

    Thus, \(\mathcal{F}\) is a \(\sigma\)-field.

    Now we define a function \(P: \mathcal{F} \to \mathbb{R}\) such that
    \(P(x) = \frac{1}{2x}, \{x\} \in \mathcal{F}\) and for other sets in \(\mathcal{F}\),
    the value of \(P\) is defined by \(P\left(\bigcup_{i} \{x_i\}\right) = \sum_{i} P(x_i)\)
    when all \(x_i\)'s are powers of 2, \(P(\varnothing) = 0\), and for a set \(A\) for which
    \(P\) is defined for \(A^c\), \(P(A) = 1 - P(A^c)\)

    First we note that \(P\) is a well defined function since we have defined \(P\) for 
    the empty set, for sets containing only powers of 2, and also for sets which are their
    complements. These being the only types of sets in \(\mathcal{F}\) implies P is a well
    defined function.

    To prove \(P\) is a probability, we need to show
    \begin{enumerate}
        \item {
            \(P(\Omega) = 1\)

            We have defined \(P(\varnothing) = 0\), which implies 
            \(P(\Omega) = 1 - P(\varnothing) = 1\)
        }
        \item {
            \(P(A) \geq 0 \, \forall A \in \mathcal{F}\)

            We note that for all sets consisting of only powers of 2, \(P(A) \geq 0\)
            by definition.
            
            Also, if \(A = \bigcup_{i} \{2^{x_i}\} \neq \varnothing\), then
            \begin{align*}
                P(A) &= P\left(\bigcup_{i} \{2^{x_i}\}\right) \\
                     &= \sum_{i} P(2^{x_i}) \tag*{(By definition)}\\
                     &\leq \sum_{i=0}^{\infty} P(2^i) \\
                     &\leq \sum_{i=0}^{\infty} \frac{1}{2 \times 2^i} = 1\\
                \therefore P(A) &\leq 1
            \end{align*}

            So, \(P(A^c) = 1 - P(A) \geq 0\) and \(P(\varnothing) = 0 \geq 0\)

            So we have shown that for all type of sets \(A \in \mathcal{F}\), \(P(A) \geq 0\)
        }
        \item {
            \(P(\bigcup_{i} A_i) = \sum_i P(A_i)\), where \(A_i\)'s are mutually disjoint events

            Here, we have two cases:
            \begin{enumerate}
                \item {
                    All \(A_i\)'s only contain powers of 2:

                    Then it is obvious that the union of the \(A_i\)'s would contain
                    all those powers of 2, and by using the definition of \(P\) can
                    be expanded to a sum, and those sums could be grouped into \(A_i\)'s
                    again. This is possible since \(A_i\)'s are disjoint.
                }
                \item {
                    There is atleast one \(A_i\) which does not contain a multiple of 2:

                    We note that there can only be one such set which does not contain a
                    multiple of 2. If there were 2 such sets, then 3 would belong to both
                    of those sets since they have to be complements of sets containing only
                    powers of 2 and that contradicts our assumption of them to be
                    disjoint.

                    Let that one set be \(\mathbb{R} \setminus \bigcup_i 2^{x_i}\). Then
                    the other sets, which contain only powers of 2 would only have elements
                    which belong to \(\bigcup_i 2^{x_i}\) for the sets to be disjoint.

                    Then we see that on applying \(P\) to the union of these sets, and expanding,
                    we would be able to add an subtract terms of \(P(x_i)\)'s for \(x_i\)'s which
                    belonged to other sets, and then group the terms into sum of application of \(P\)
                    on all the sets
                }
            \end{enumerate}
        }
    \end{enumerate}

    Thus, \(P\) defines a probability on \(\mathcal{F}\) and it's easily seen that \(P\)
    has infinite number of non-zero values.
}


\item {
    Random Variable/Function of a Random Variable

    Alice is trying to send \(X\) bits of data to Bob, where \(X \sim P(5)\). However, during
    transmission there is a \(10\%\) chance for each bit to flip. What is the probability
    that Bob receives incorrect data? Now suppose Alice also sends a parity bit, which is
    0 if there are even number of bits equal to 1, and 1 otherwise; and Bob then checks the
    data with the parity bit upon receiving i.e. if he receives data with 3 bits set and
    parity bit 0, he'll know the data is erroneous. What is the probability the Bob receives
    data which is erroneous and also matches the information given by the parity bit?

    \textbf{Solution}

    \underline{Without the parity bit}

    If \(X = k\), then the probability of successful transmission is \((1 - 0.1)^k = 0.9 ^ k\),
    which means probability of error in transmission is \(1 - 0.9^k\)

    \(\therefore\) By the total probability rule,
    \begin{align*}
        P(\text{error in transmission}) &= \sum_{k = 0}^{\infty} 
                        P(\text{error in transmission } |\,X = k) \times P(X = k) \\
            &= \sum_{k = 0}^{\infty} \{1 - 0.9^k\} \frac{e^{-5}\,5^k}{k!} \\
            &= \sum_{k = 0}^{\infty} \frac{e^{-5}\,5^k}{k!} - \sum_{k = 0}^{\infty} 0.9^k\,\frac{e^{-5}\,5^k}{k!} \\
            &= e^{-5}\,\sum_{k = 0}^{\infty} \frac{5^k}{k!} - \sum_{k = 0}^{\infty} \frac{e^{-5}\,4.5^k}{k!} \\
            &= 1 - e^{-5}\,e^{0.9 \times 5}  \tag*{(Using Taylor series of \(e^x\))} \\
            &= 1 - e^{-0.5} \\
            &\approxeq 0.39
    \end{align*}

    \underline{With the parity bit}

    Let \(p = 0.1\) be the chance that a bit flips

    If \(X = k\), then there are two cases to consider, namely, if the parity bit changes
    during transmission or the parity bit remains the same.

    \begin{enumerate}
        \item {
            Parity bit remains the same:

            For the data to be changed and still match the information given by the parity bit,
            we note that an non-zero and even number of bits must be flipped, so the number of
            bits equal to 1 modulo 2 remains the same.

            So, in this case,
            \[
                P(\text{undetectable error}) = {k \choose 2}\,p^2\,(1-p)^{k-2} + {k \choose 4}\,p^4\,(1-p)^{k-4} + \cdots
            \]
        }
        \item {
            Parity bit flips:

            For the data to be changed and match the parity bit received by Bob, we observe that
            an even number of bits in the data must be flipped, so the number of bits equal to 1
            modulo 2 changes from the original data

            \(\therefore\) In this case,
            \[
                P(\text{undetectable error}) = {k \choose 1}\,p^1\,(1-p)^{k-1} + {k \choose 3}\,p^3\,(1-p)^{k-3} + \cdots
            \]
        }
    \end{enumerate}

    We note that the probability of the parity bit flipping is \(p\), and so combining the cases,
    \begin{align}
        P(\text{undetectable error}) &= (1 - p) \,
            \left[{k \choose 2}\,p^2\,(1-p)^{k-2} + {k \choose 4}\,p^4\,(1-p)^{k-4} + \cdots\right] \nonumber \\
            &+ p \,
            \left[{k \choose 1}\,p^1\,(1-p)^{k-1} + {k \choose 3}\,p^3\,(1-p)^{k-3} + \cdots\right]
            \label{eq:1}
    \end{align}

    To solve these equations, we use the binomial theorem as follows:
    \begin{align*}
        (x + y)^k &= {k \choose 0}\,x^0\,y^k + {k \choose 1}\,x^1\,y^{k-1} + \cdots + {k \choose k}\,x^k\,y^0 \\
        (y - x)^k &= {k \choose 0}\,x^0\,y^k - {k \choose 1}\,x^1\,y^{k-1} + \cdots + (-1)^k \, {k \choose k}\,x^k\,y^0 
    \end{align*}

    Adding and subtracting the above two equations, and replacing \(x\) by p and \(y\) by \(1-p\), we get
    \begin{align}
        \frac{1}{2}\left\{(p + 1 - p)^k + (1 - p - p)^k\right\} &= {k \choose 0}\,p^0\,(1-p)^k + {k \choose 2}\,p^2\,(1-p)^{k-2} + \cdots \label{eq:2}\\
        \frac{1}{2}\left\{(p + 1 - p)^k - (1 - p - p)^k\right\} &= {k \choose 1}\,p^1\,(1-p)^{k-1} + {k \choose 3}\,p^3\,(1-p)^{k-3} + \cdots \label{eq:3}
    \end{align}

    From equations \ref{eq:1}, \ref{eq:2} and \ref{eq:3}, we get
    \begin{align*}
        P(\text{undetectable error}) &= (1 - p)\,\left[\frac{1}{2}\left\{1 + (1 - 2p)^k\right\} - 
                {k \choose 0}\,p^0\,(1-p)^k\right] \\
                &+ p\,\left[\frac{1}{2}\left\{1 - (1 - 2p)^k\right\}\right] \\
            &= \frac{1}{2} - (1 - p)^{k + 1} + \frac{1}{2}\,(1 - 2p)^{k+1} \\
        \intertext{Replacing \(p\) by 0.1, we get}
        P(\text{undetectable error}) &= \frac{1}{2} - 0.9^{k + 1} + \frac{1}{2}\,0.8^{k+1}
    \end{align*}

    Now by the total probability rule,
    \begin{align*}
        P(\text{undetected error}) &= \sum_{k = 0}^{\infty} 
                        P(\text{undetected error } |\,X = k) \times P(X = k) \\
            &= \sum_{k = 0}^{\infty} 
                \left[\frac{1}{2} - 0.9^{k + 1} + \frac{1}{2}\,0.8^{k+1}\right]
                \frac{e^{-5}\,5^k}{k!} \\
            &= \frac{1}{2}e^{-5}\sum_{k = 0}^{\infty}\frac{5^k}{k!} 
               - 0.9 \times e^{-5} \sum_{k = 0}^{\infty} \frac{4.5^k}{k!}
               + 0.8 \times \frac{1}{2} e^{-5} \sum_{k = 0}^{\infty} \frac{4^k}{k!} \\
            &= \frac{1}{2} - 0.9 \times e^{-5}\,e^{4.5} + 0.4 \times e^{-5}\,e^{4} \tag*{(Using Taylor series of \(e^x\))}\\
            &= \frac{1}{2} - 0.9 \times e^{-0.5} + 0.4 \times e^{-1} \\
            &\approxeq  0.10
    \end{align*}
}

\item Two Dimensional Random Variables

\item Two Dimensional Random Variables

\item {
    Higher Dimensional Random Variables

    \begin{enumerate}
        \item Let \(A, B, C\) be 3 random variables such that their pdf is given by the function
        \[
            f(a, b, c) = \begin{cases}
                            1 & 0 < a < 1, 0 < b < 1, 0 < c < 1 \\
                            0 & \text{otherwise}
                         \end{cases}
        \]
        Find the CDF of \(A^2 + B^2 - C\) in terms of \(f\)
        \item Consider the equation \(x^2 + y^2 + 2Ax + 2By + C = 0\). Find the probability
        that this equation represents a real circle (i.e. radius \(> 0\)).
    \end{enumerate}

    \textbf{Solution}

    Let \(R = A^2 + B^2 - C\)

    We note that \(min(R) = -1\) and \(max(R) = 1\) since the maximum and minimum values
    of \(A, B, C\) are 1 and 0 respectively

    \(\therefore P(R \leq -1) = 0 \text{ and } P(R \leq 1) = 1\)

    Since \(A^2 + B^2 - C <= r \implies A^2 + B^2 <= C + r\), it makes sense to make
    cases on \(r\) with breakpoints \(r = -1\) since it gives a lower limit to the value
    \(C\) could take, \(r = 0\) since it limits the value of \(C + r\), and then finally
    at \(r = 1\)

    \begin{enumerate}
        \item {
            \(-1 \leq r < 0\)

            Since \(A^2 + B^2 = C + r\), it must be positive. So \(C\) must range from
            \(1 - r\) to \(1\). 

            For \(C = c\) in the said range, \(0 \leq c + r < 1\) and thus \(A\) can
            range from 0 to \(c + r\) and similarly for \(B\).

            \(\therefore\) In this case,
            \[
                P(R \leq r) = \int_{c = -r}^{1} \int_{a=0}^{\sqrt{c + r}} 
                              \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
            \]
        }

        \item {
            \(0 \leq r < 1\)

            \(A^2 + B^2 \leq C + r\) gives a breakpoint at \(1 - r\) for \(C\) since
            for the former, \(A\) can go from \(0 \text{ to } c + r\) but not for the
            latter since \(c + r\) would become greater than 1.

            \begin{itemize}
                \item {
                    \(0 < c < 1 - r\)

                    As described above, for this case, \(A\) will range from 0 to 
                    \(\sqrt{c + r}\) and similarly for B.
                    So the required probability is:
                    \[
                        \int_{c = 0}^{1 - r} \int_{a=0}^{\sqrt{c + r}} 
                        \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
                    \]
                }

                \item {
                    \(1 - r \leq c < 1\)

                    For this case \(c + r > 1\), and \(A\) can range fully from 0 to 1
                    Then, considering \(B^2 \leq c + r - a^2\) we note that when 
                    \(a > \sqrt{c + r - 1}\), \(B\) ranges from 0 to \(\sqrt{c + r - a^2}\)
                    and when \(0 < a leq \sqrt{c + r - 1}\), \(B\) ranges from 0 to 1.

                    \(\therefore\) The required probability is:
                    \[
                        \int_{c = 1 - r}^{1} \int_{a=0}^{\sqrt{c + r - 1}} 
                        \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc
                        + \int_{c = 1 - r}^{1} \int_{\sqrt{c + r - 1}}^{1} 
                        \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
                    \]
                }
            \end{itemize}

            Totalling up the probabilities for this case, we have
            \begin{align*}
                P(R \leq r) &= \int_{c = 0}^{1 - r} \int_{a=0}^{\sqrt{c + r}} 
                \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc \\
                &+ \int_{c = 1 - r}^{1} \int_{a=0}^{\sqrt{c + r - 1}} 
                \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc \\
                &+ \int_{c = 1 - r}^{1} \int_{\sqrt{c + r - 1}}^{1} 
                \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
            \end{align*}
        }

        \item {
            \(1 \leq r < 2\)

            \(A^2 + B^2 \leq C + r\) gives the breakpoint at \(2 - r\) for \(C\) since
            for the part before the breakpoint, \(A\) and \(B\) can't take full values,
            but for \(c \geq 2 - r\), we get \(c + r \geq 2\) which means both \(A\) and \(B\)
            can be from 0 to 1.

            \begin{itemize}
                \item {
                    \(0 < c < 2 - r\)

                    For this case, \(1 \leq c + r < 2\), so \(A\) can fully range from
                    0 to 1. However, from \(B^2 \leq c + r - A^2\), when \(A\) is from 0 to
                    \(\sqrt{c + r - 1}\), \(B\) can fully range from 0 to 1 but not in the
                    remaining interval.

                    \begin{itemize}
                        \item {
                            \(0 < a < \sqrt{c + r - 1}\)

                            As noted above, \(B\) can range from 0 to 1, so the required probability
                            is:
                            \[
                                \int_{c = 0}^{2 - r} \int_{a=0}^{\sqrt{c + r - 1}} 
                                \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc
                            \]
                        }
                        \item {
                            \(\sqrt{c + r - 1} \leq a < 1\)

                            Here, \(c + r - a^2 \leq 1\), so \(B\) will range from 0 to
                            \(\sqrt{c + r - a^2}\). So the required probability is:
                            \[
                                \int_{c = 0}^{2 - r} \int_{a=\sqrt{c + r - 1}}^{1} 
                                \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc
                            \]
                        }
                    \end{itemize}
                }
                \item {
                    \(2 - r \leq c < 1\)

                    Here, as we already noted, \(c + r \geq 2\), so \(A\) and \(B\) can fully
                    range from 0 to 1. So the required probability is:

                    \[
                        \int_{c = 2 - r}^{1} \int_{a=0}^{1} 
                        \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc
                    \]
                }
            \end{itemize}

            Totalling up the probabilities for this case, we have
            \begin{align*}
                P(R \leq r) &= \int_{c = 0}^{2 - r} \int_{a=0}^{\sqrt{c + r - 1}} 
                                \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc \\
                            &+ \int_{c = 0}^{2 - r} \int_{a=\sqrt{c + r - 1}}^{1} 
                                \int_{b = 0}^{\sqrt{c + r - a^2}} f(a, b, c)\, db\, da\, dc \\
                            &+ \int_{c = 2 - r}^{1} \int_{a=0}^{1} 
                                \int_{b = 0}^{1} f(a, b, c)\, db\, da\, dc
            \end{align*}
        }
    \end{enumerate}

    The probabilities as calculated in the different cases can be stated together and
    thus would define the CDF completely.

    Given the equation \(x^2 + y^2 + 2Ax + 2By + C = 0\), we can rearrange it to
    \((x - A)^2 + (y - B)^2 = A^2 + B^2 - C\), which as we observe is a circle with the
    centre \((A, B)\) and square of the radius equal to \(A^2 + B^2 - C\)

    We need to find \(P(A^2 + B^2 - C > 0)\). Taking the expression of \(P(R \leq r)\)
    corresponding to \(r = 0\) and substituting \(r = 0\), we get
    \begin{align*}
        P(A^2 + B^2 - C \leq 0) &= \int_{c = 0}^{1} \int_{a=0}^{\sqrt{c}} 
                                    \int_{b = 0}^{\sqrt{c - a^2}} f(a, b, c)\, db\, da\, dc \\
                                &= \int_{0}^{1} \int_{0}^{\sqrt{c}} 
                                \int_{0}^{\sqrt{c - a^2}} \, db\, da\, dc \\
                                &= \int_{0}^{1} \int_{0}^{\sqrt{c}} \sqrt{c - a^2}\, da\, dc \\
                                &= \int_{0}^{1} \left[\frac{a}{2}\sqrt{c - a^2} + \frac{c}{2}
                                    \sin^{-1}{\frac{a}{\sqrt{c}}}\right]^{\sqrt{c}}_{0} dc \\
                                &= \int_{0}^{1} \frac{c}{2} \, \frac{\pi}{2} \, dc \\
                                &= \frac{\pi}{8}
    \end{align*}

    \(\therefore P(A^2 + B^2 - C > 0) = 1 - P(A^2 + B^2 - C \leq 0) = 1 - \dfrac{\pi}{8}\)
}

\item Higher Dimensional Random Variables

\item {
    Cross Moments

    Two points are chosen randomly on the perimeter of a square with sides of unit length.
    Find the expected value of the distance between the two points.

    \textbf{Solution}

    We observe that there can be 3 cases for the points with respect to the positions at
    which they lie on the perimeter, namely, they can lie either on the same side, adjacent
    sides or on opposite sides.

    In the following cases, we calculate the conditional expectation of the distance between
    the points given that case:

    \begin{enumerate}
        \item {
            Points lie on the same side

            Let \(E_1\) be the event that both points, say \(A\) and \(B\), lie on
            the same side of the square. Let their distance from an endpoint of the edge
            be \(X\) and \(Y\) respectively, so distance between them is \(|X - Y|\)

            We observe that the conditional probability distribution function,
            \(p_{X, Y / E_1} = 1\) 

            Then, \[
                E(\text{Distance between the points} \,|\, E_1) = \int_{0}^{1} \int_{0}^{1}
                   |x - y| \,dy \,dx = \frac{1}{3}
            \]
        }

        \item {
            Points lie on adjacent sides

            Let \(E_2\) be the event that both points, say \(A\) and \(B\), lie on
            adjacent sides of the square. Let their distance from the vertex common to
            both the edges be \(X\) and \(Y\) respectively, so distance between them is
            \(\sqrt{X^2 + Y^2}\)

            We observe that the conditional probability distribution function,
            \(p_{X, Y / E_1} = 1\) 

            Then. \[
                E(\text{Distance between the points} \,|\, E_2) = \int_{0}^{1} \int_{0}^{1}
                \sqrt{x^2 + y^2} \,dy \,dx \approx 0.7652
            \]
            \hfill (Note: Calculated using numerical integration tools)
        }

        \item {
            Points lie on opposite sides

            Let \(E_3\) be the event that both points, say \(A\) and \(B\), lie on
            opposite sides of the square. Let their distance from the vertices on the
            same side be \(X\) and \(Y\) respectively, so distance between them is
            \(\sqrt{(X - Y)^2 + 1}\)

            We observe that the conditional probability distribution function,
            \(p_{X, Y / E_1} = 1\) 

            Then, \[
                E(\text{Distance between the points} \,|\, E_3) = \int_{0}^{1} \int_{0}^{1}
                   \sqrt{(x-y)^2 + 1} \,dy \,dx \approx 1.0766
            \]
            \hfill (Note: Calculated using numerical integration tools)
        }
    \end{enumerate}

    Now, we calculate the probabilities of the events \(E_1, E_2, E_3\)

    \begin{enumerate}
        \item {
            \(E_1\) is the event that both points lie on the same side of the square.
            The first point can be on any side and for the second point to be on
            the same side, the probability is \(\frac{1}{4}\)

            \[\therefore P(E_1) = \frac{1}{4}\]
        }
        \item {
            \(E_2\) is the event that both points lie on adjacent sides of the square.
            The first point can be on any side and the second point has 2 choices of
            adjacent sides, so the probability is \(\frac{2}{4} = \frac{1}{2}\)

            \[\therefore P(E_2) = \frac{1}{2}\]
        }
        \item {
            \(E_3\) is the event that both points lie on opposite sides of the square.
            The first point can be on any side and the second point has 1 choice for an
            opposite side, so the probability is \(\frac{1}{4}\)

            \[\therefore P(E_3) = \frac{1}{4}\]
        }
    \end{enumerate}

    Now, we calculate the expected distance between the two points
    \begin{align*}
        E(\text{Distance between the points}) &= P(E_1) \cdot E(\text{Distance between the points} \,|\, E_1) \\
                                              &+ P(E_2) \cdot E(\text{Distance between the points} \,|\, E_2) \\
                                              &+ P(E_3) \cdot E(\text{Distance between the points} \,|\, E_3) \\
                                              &\approx \frac{1}{4} \times \frac{1}{3} + \frac{1}{2} \times 0.7652
                                                   + \frac{1}{4} \times 1.0766 \\
                                              &\approx 0.7351
    \end{align*}
}

\item Cross Moments

\item {
    Limiting Distributions

    A spaceship has an initial velocity of zero and for each interval of 1 second,
    either accelerates at 1 meter per square second with probability 0.4 or does not 
    accelerate with probability 0.6. Estimate the probability of the total distance covered
    in 2 minutes being less than 5 kilometers.

    \textbf{Solution}

    Since the decision of accelerating or not happens at interval of 1 seconds, we will
    work in these intervals. So, we need to find total distance covered in 120 intervals.

    Let \(X_i\) denote the increment in speed from the beginning of an interval to the
    end of the interval. Then, we note that \(X_i\)'s are independent and 
    \[
        P(X_i = x) = \begin{cases}
                        0.6 & x = \text{acceleration} \times \text{time} = 1 \\
                        0.4 & x = 0
                     \end{cases}
    \]

    Therefore, the speed at the starting of the \(n^{th}\) interval is 
    \(\sum_{i=1}^{n-1} X_i\)

    Given the increment of speed \(X_n\) in then \(n^{th}\) interval, the contribution of the
    acceleration to the distance travelled in the interval is
    \[\frac{1}{2} \times \text{acceleration} \times \text{time}^2 = \frac{1}{2} X_n\]

    Therefore, the distance travelled in the \(n^{th}\) interval is given by
    \[
        d_n = \sum_{i=1}^{n-1} X_i + \frac{1}{2} X_n
    \]

    Thus, the total distance travelled in 2 minutes i.e. 120 intervals is
    \begin{align*}
        D &= \sum_{n = 1}^{120} d_n \\
          &= \sum_{n = 1}^{120} \left[\sum_{i=1}^{n-1} X_i + \frac{1}{2} X_n\right] \\
          &= \sum_{n = 1}^{120} \sum_{i=1}^{n-1} X_i + \sum_{n = 1}^{120} \frac{1}{2} X_n \\
          &= \sum_{n = 1}^{119} (120 - n) X_n + \sum_{n = 1}^{120} \frac{1}{2} X_n 
                          \tag*{(Expanding the double summation)} \\   
          &= \sum_{n = 1}^{119} (120.5 - n) X_n + \frac{1}{2}X_{120}
    \end{align*}

    Let \(X = \sum_{n = 1}^{119} (120.5 - n) X_n\). Since \(X_n\)'s are independent,
    then \(X\) is a sum of a large number of independent random variables and by the
    Central Limit Theorem must be normally distributed

    \begin{align*}
        E\left[\sum_{n = 1}^{119} (120.5 - n) X_n\right] &= \sum_{n = 1}^{119} (120.5 - n) E(X_n) \\
            &= \sum_{n = 1}^{119} (120.5 - n) (0.6 \times 1 + 0.4 \times 0) \\
            &= \sum_{n = 1}^{119} (120.5 - n) \times 0.6 \\
            &= 120.5 \times 0.6 \times 119 - 0.6 \times \frac{119 \times 120}{2} \\
            &= 4319.7
    \end{align*}

    \begin{align*}
        Var\left[\sum_{n = 1}^{119} (120.5 - n) X_n\right]
            &= \sum_{n = 1}^{119} Var\left[(120.5 - n) X_n\right] \tag*{(As the rv's are independent)} \\
            &= \sum_{n = 1}^{119} (120.5 - n)^2 \, Var(X_n) \\
            &= \sum_{n = 1}^{119} (120.5 - n)^2 \, [0.6 \times (1 - 0.6)^2 + 0.4 \times (0 - 0.6)^2] \\
            &= 138237.54
    \end{align*}

    The total distance travelled in 120 intervals is \(D = X + \frac{1}{2} X_{120}\). Then 
    \begin{align*}
        P(D \leq 5000) &= P\left(X + \frac{1}{2} X_{120} \leq 5000\right) \\
        \intertext{Since \(X\) is combination of \(X_1\) to \(X_{119}\) and \(X_{i}\)'s 
                    are independent, \(X\) and \(X_{120}\) are independent}
        \implies P(D \leq 5000) &= P(X_{120} = 1) P(X \leq 4995.5) + P(X_{120} = 0) P(X \leq 2000) \\
            &= 0.6 \times P(X \leq 4995.5) + 0.4 \times P(X \leq 5000)
    \end{align*}

    From above discussion, we know that \(X\) is normally distributed with mean 4319.7 
    and variance 138237.54. This implies that
    \[\frac{X - 4319.7}{\sqrt{138237.54}} = \frac{X - 4319.7}{371.8} \sim \mathcal{N}(0, 1)\]
    \[\text{In other words, } X = 371.8 Z + 4319.7 \text{ where } Z \sim \mathcal{N}(0, 1)\]

    Let \(\Phi(z)\) be the CDF of Z i.e. \(\Phi(z) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{z} e^{-t^2/2} \,dt\)

    Then
    \begin{align*}
        P(D \leq 5000) &= 0.6 \times P(X \leq 4995.5) + 0.4 \times P(X \leq 5000) \\
            &= 0.6 \times P(371.8 Z + 4319.7 \leq 4995.5) + 0.4 \times P(371.8 Z + 4319.7 \leq 5000) \\
            &= 0.6 \times P(Z \leq 1.818) + 0.4 \times P(Z \leq 1.830) \\
            &= 0.6 \times 0.96562 + 0.4 \times 0.96638 \\
            &\approx 0.97
    \end{align*}

}

\item {
    Limiting Distributions

    For an estimation, take number of people in a factory to be given by the random variable
    \(N \sim P(\mu)\). Total time taken off by a worker is assumed to be independent of each other
    and following the distribution \(Exp(\lambda (an + b))\). Suppose
    the factory owner decides the downsize the workforce and fire all the workers who have taken
    time off for time more than \(t\), estimate the expected number of workers who will be fired.
    Assume \(\mu\) to be large, i.e. practically the factory has a large number of workers

    \textbf{Solution}

    Let \(T\) be the time off taken by an employee, then
    \[
        P(T > t) = 1 - P(T \leq t) = 1 - (1 - e^{-\lambda t(an + b)}) = e^{-\lambda t(an + b)}
    \]

    For a given \(N = n\), we assume that n is large, and applying the Bernoulli law of
    large numbers, we get that \(\frac{n_f}{n} \approx e^{-\lambda t(an + b)}\), 
    where \(n_f\) is the number of people fired.

    So, \(n_f = n e^{-\lambda t(an + b)}\) when factory has \(n\) workers

    \begin{align*}
        E(n_f) &= E(E(n_f | N)) \\
               &= E(N e^{-\lambda t(aN + b)}) \\
               &= \sum_{n = 0}^{\infty} n e^{-\lambda t(an + b)} \frac{e^{-\mu} \mu^n}{n!} \\
               &= \mu e^{-\mu} \sum_{n = 1}^{\infty} e^{-\lambda t(an + b)} \frac{\mu^{n-1}}{(n-1)!} \\
               &= \mu e^{-\mu} \sum_{n = 0}^{\infty} e^{-\lambda t(an + (a + b))} \frac{\mu^{n}}{n!} \\
               &= \mu e^{-\mu} e^{-\lambda t(a + b)} \sum_{n = 0}^{\infty} \frac{(\mu e^{-a \lambda t})^n}{n!} \\
               &= \mu e^{-\mu} e^{-\lambda t(a + b)} e^{\mu e^{-a \lambda t}} \\
               &= \mu e^{\mu(e^{-a \lambda t} - 1) - \lambda t (a + b)}
    \end{align*}
}



\end{enumerate}

\end{document}
